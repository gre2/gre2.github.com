---
layout: post
title: "rocketmq浅识"
description: ""
category: [java,rocketmq]
tags: [rocketmq]
---
{% include JB/setup %}

# ROCKETMQ浅识

### 0.关注点

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kgqyrsxj211w0d80yb.jpg)

### 1.架构图

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kh80uw2j20ld08eta9.jpg)

broker向nameser周期性注册，包含信息：broker的信息，id，ip，ha地址；自身上topic的信息，topic的名称，包含多少messageQueue，生产者从namser上找到自己想要生产的路由topic信息，根据路由信息找到指定的broker，找到指定的topic，找到指定的messageQueue进行发送；消费者从里面拿到路由信息进行消费；

### 2.整体架构图

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kr5oyx8j20kl095q70.jpg)

消费组之间消费进度独立，topic被分成多个messageQueue，为并行消费提供可能

### 3.概念介绍

```
Broker:消息队列的服务端，消息实际存储的地方
NameServer：服务发现装置，broker注册到nameServer上，producer和consumer通过nameServer发现broker的地址
Topic:每个消息都属于某个主题，消息在消息队列按主题分离
ConsumerGroup：消费组，多个消费者组成一个组，共同消费某个topic，可以起到组内负载均衡的作用
MQ:看做消息传递的媒介，生成者投递消息到MQ(消息队列)，消费者从MQ（消息队列）获取消息
ConsumeQueue：每个message queue一个，作为消费消息的索引，保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量offset，消息大小size和消息Tag的HashCode值。
```

### 4.生成消费流程图

![](http://ww1.sinaimg.cn/large/87a42753ly1g34khyhmbxj20jf0ca0w4.jpg)

messageQueue只是存储了消息的索引，数据存在一个broker的commit log中

### 5.rpc通信

基于netty，nio

### 6.发-存-收

存：Producer端发送消息最终写入的是CommitLog（消息存储的日志数据文件），Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储

发：Producer发送消息至Broker端，然后Broker端使用同步或者异步的方式对消息刷盘持久化，保存至CommitLog中。只要消息被刷盘持久化至磁盘文件CommitLog中，那么Producer发送的消息就不会丢失。

收：Consumer端先从ConsumeQueue（消息逻辑队列）读取持久化消息的起始物理位置偏移量offset、大小size和消息Tag的HashCode值，随后再从CommitLog中进行读取待拉取消费消息的真正实体内容部分；

### 7.rocketmq架构优缺点

发送消息时，生产者端的消息确实是**顺序写入CommitLog**；订阅消息时，消费者端也是**顺序读取ConsumeQueue**，然而根据其中的起始物理位置偏移量offset读取消息真实内容却是**随机读取CommitLog**。

 （1）**优点：**
a、ConsumeQueue消息逻辑队列较为轻量级；
b、对磁盘的访问串行化，避免磁盘竟争，不会因为队列增加导致IOWAIT增高；
（2）**缺点：**
a、对于CommitLog来说写入消息虽然是顺序写，但是读却变成了完全的随机读；
b、Consumer端订阅消费一条消息，需要先读ConsumeQueue，再读Commit Log，一定程度上增加了开销；

### 8.拉取方式的改进

push

由消息中间件主动将消息推给消费者，但是在消息处理能力较弱的时候，消费者端的缓冲区可能溢出，造成异常

pull

由消费者客户端主动向消息中间件拉取消息，如果pull的时间间隔比较久，会增加消息延迟；pull的时间间隔短，在一个时间段内没有消息可以消费，产生很多无效的pull请求，影响mq性能

长轮询pull

消费者第一次pull消息失败，并不立即给消费者客户端返回response的响应，而是hold住并且挂起请求，然后使用PullRequestHoldService从pullRequestTable本地缓存变量中不断地去取，当待拉取消息的偏移量小于消息队列最大的偏移量【说明有消息到broker】，则通过重新调用一次业务处理器—PullMessageProcessor的处理请求方法—processRequest()来重新尝试拉取消息（此处，每隔5S重试一次，默认长轮询整体的时间设置为30s）。

![](https://ws1.sinaimg.cn/large/87a42753ly1fwro3yhw1vj20wt0hdn3o.jpg)

### 9.pageCache与Mmap内存映射

系统的所有文件I/O请求，操作系统都是通过page cache机制实现的，对于操作系统来说，磁盘文件都是由一系列的数据块顺序组成。

操作系统内核在处理文件I/O请求时，首先到page cache中查找（page cache中的每一个数据块都设置了文件以及偏移量地址信息），如果未命中，则启动磁盘I/O，将磁盘文件中的数据块加载到page cache中的一个空闲块，然后再copy到用户缓冲区中。
page cache本身也会对数据文件进行预读取，对于每个文件的第一个读请求操作，系统在读入所请求页面的同时会读入紧随其后的少数几个页面。因此，想要提高page cache的命中率（尽量让访问的页在物理内存中），从硬件的角度来说肯定是物理内存越大越好。从操作系统层面来说，访问page cache时，即使只访问1k的消息，系统也会提前预读取更多的数据，在下次读取消息时, 就很可能可以命中内存。

在RocketMQ中，ConsumeQueue逻辑消费队列存储的数据较少，并且是顺序读取，在page cache机制的预读取作用下，Consume Queue的读性能会比较高近乎内存，即使在有消息堆积情况下也不会影响性能。

 另外，RocketMQ主要通过MappedByteBuffer对文件进行读写操作。其中，利用了NIO中的FileChannel模型直接将磁盘上的物理文件直接映射到用户态的内存地址中，这样程序就好像可以直接从内存中完成对文件读/写操作一样。只有当缺页中断发生时，直接将文件从磁盘拷贝至用户态的进程空间内，只进行了一次数据拷贝。对于容量较大的文件来说（文件大小一般需要限制在1.5~2G以下），采用Mmap的方式其读/写的效率和性能都非常高。

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kicgwinj20ji09umyt.jpg)

 PageCache机制也不是完全无缺点的，当遇到OS进行脏页回写，内存回收，内存swap等情况时，就会引起较大的消息读写延迟，优化方式主要包括内存预分配，文件预热和mlock系统调用。

### 10.消费模型

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kin8n3zj20im0bftci.jpg)

### 11.失败，重试

```
生产者内部都有发送失败会自动进行重试，重试次数可配置。对于同步发送，可以通过配置项setRetryTimesWhenSendFailed(int retryTimesWhenSendFailed)来设置发送重试次数。对于异步发送，可以通过配置项setRetryTimesWhenSendAsyncFailed(final int retryTimesWhenSendAsyncFailed)来设置重试次数。这个自动重试可能造成消息重复，需要注意。通过参数可以设置自动重试，但是自动重试有可能造成消息的重复。
```

```
业务逻辑执行失败时，返回RECONSUME_LATER，表示消息处理失败需要重新消费。消息会被发送回Broker，等待一段时间后被Consumer再次获取。
```

 **（1）重试队列：**如果Consumer端因为各种类型异常导致本次消费失败，为防止该消息丢失而需要将其重新回发给Broker端保存，保存这种因为异常无法正常消费而回发给MQ的消息队列称之为重试队列。RocketMQ会为每个消费组都设置一个Topic名称为**“%RETRY%+consumerGroup”的重试队列**（这里需要注意的是，**这个Topic的重试队列是针对消费组，而不是针对每个Topic设置的**）。

 **（2）死信队列：**由于有些原因导致Consumer端长时间的无法正常消费从Broker端Pull过来的业务消息，为了确保消息不会被无故的丢弃，那么超过配置的“最大重试消费次数”后就会移入到这个死信队列中，RocketMQ会为每个消费组都设置一个Topic命名为**“%DLQ%+consumerGroup"的死信队列**。一般在实际应用中，移入至死信队列的消息，需要人工干预处理；

### 12.有序

consumer有两种消费模式，consumer.registerMessageListener(new MessageListenerConcurrently() {...})或者consumer.registerMessageListener(new MessageListenerOrderly() {...})。前者是并发的，是视频里的模型，实现的时候是这样的，它设了一个大小为n的线程池，假如你一次拉了100条数据，设置每批大小为10，会把消息封装成100/10=10个任务提交到线程池执行。后者是顺序消费时使用的，它消费的时候是使用单线程消费的。实现的时候是这样的，它设了一个大小为n的线程池，假如你一次拉了100条数据，会把100条消息封装成1个任务提交到线程池执行【一个任务只会有一个线程执行】。如果你要用顺序消息，要使用MessageListenerOrderly。 

### 13.丢消息

1.消息未被消费，存放消息的磁盘发生不可逆的损坏

2.消息在系统pageCache内，且未被消费，服务器断电

解决方案：

1.开启同步复制

2.开启同步刷盘

### 14.重复消费

1. Kill -9直接终止consume进程，消费进度未被提交到broker
2. 有consume重启，group内发生rebalance时，部分消息消费进度未提交
3. 生产者多次发送一条相同的消息（broker写入成功，但是返回写到producer超时，生成者重试）

### 15.复制，刷盘

```
CPU>RAM>DISK
速度和容量折中
cpu-ram    cpucache  高速缓存
ram-disk   pagecache 
复制：master-slave
刷盘：pagecache-disk
同步：主从都完成
异步：主完成
```

### 16.消息高可靠

数据通过磁盘进行持久化的存储

### 17.高可用

消费端：当master不可用或者繁忙的时候，consumer会自动切换到slave读

发送端：把topic的多个messagequeue创建在多个broker上，当一个组的master不可用，其他组的master仍然可以用，rocketmq目前不支持slave自动转换成master，需要改配置文件。

### 18.ack

```
1.success，消费成功，，更新进度
2.reconsume—later：失败，送回broker（retry队列），更新进度
3.null:打日志，置为reconsume—later
4.异常
5.hang住不返回，consume一直被block

注：
全hang住，消息有个阈值在内存，所有不再消费
定时任务，工作线程，15分钟检查，获取hang住的consume，送回broker，把消费进度更新掉，正常消费了
```

### 19.并行ACK

提交自己messageQueue未被处理的最小的offset到本地内存

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kiz3mkgj20pe0gmgsj.jpg)

 consumeThread_1一直没有处理成功，定时任务（超时时间=15分钟）检查阻塞一直没有返回成功处理的消息，把消息送回broker（retry队列，非messageQueue那个队列），并且更新进度

### 20.consumer何时提交offset到broker

1.周期性任务5s

2.shutdown

3.rebalance发现queue不在分配给自身时

### 21.broker如何存储offset

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kj92g8fj20ps0c3n1o.jpg)

红色是queueId

### 22.消费组里的消费者如何均分topic下的messageQueue

```
consume有个rebalance过程：把messageQueue列表分配到consume列表的一个过程
相关接口：AllocateMessageQueueStrategy
```

### 23.数据保存时间

无论是否消费过，broker上的数据超过数据保存时间后会被自动清理，默认72h

### 24.新来的消费者组从哪里开始消费

用户启动Consumer时，可以通过方法`setConsumeFromWhere(consumeFromWhere)`来配置消费组从何处开始消费。ConsumerFromWhere提供以下选择：

- CONSUME_FROM_LAST_OFFSET 从最后的消息开始消费

- CONSUME_FROM_FIRST_OFFSET 从最前的消息开始消费

- CONSUME_FROM_TIMESTAMP 从指定的时间点开始消费



### 25.发现消费者不消费时，该如何诊断？

首先确认消费者的连接状态，是否正常连接Broker。如果连接没有问题，看消息是否该消费者是否有分配到ConsumeQueue，如果未分配到，一般是由于消费者数目超过了ConsumeQueue的总数。如果分配到了，看该分配到的ConsumeQueue上是否有消息生产进来，通过看**brokerOffset和consumerOffset**就可以知道是消息未生产，还是消费者不消费。如果确认是消费者连接正常，分配正常，生产者生产正常，那么需要查看消费者的客户端日志再进一步分析。



### 26.对于慢消费的影响和amq比较

ActiveMQ对于慢消费者的容忍度很低，目前使用上观察到ActiveMQ会自动Kill慢消费者。对于非持久化的消息，慢消费者会导致Broker释放不掉内存，同时影响消费和生产。对于持久化的消息，消息可以先存储下来，但是如果消费者落后太多仍然会影响性能。

更为关键的是，在桥接场景中，如果Broker A认为Broker B的代理消费者为慢消费者，则桥接会断开。从而导致Broker A和Broker B之间消息无法正常流转。影响正常的消息消费。

RocketMQ对于慢消费者则不会很敏感。由于默认数据都是持久化的，不会由于慢消费者没有ACK消息而对Broker的内存产生任何影响。同时，如果慢消费者消费落后很多，消费很早以前的消息，可能会对Broker的性能产生一些影响，因此，Broker可以配置阈值，组织消费多少字节之前（默认16G）的消息的消费者。



### 27.可扩展性和amq比较

ActiveMQ基本不具有横向扩展的能力，如果发现ActiveMQ的吞吐已经无法满足当前业务场景，只有选择纵向扩展。也就是切换成性能更好的机器重新部署，而无法通过增加机器进行横向扩展。

RocketMQ本身就是分布式设计，从Broker到Producer/Consumer都可以集群化部署。如果发现集群吞吐到达瓶颈，只需要通过新增机器就可以达到横向扩容，全程透明。

### 28.HA和amq比较

![](http://ww1.sinaimg.cn/large/87a42753ly1g34kjkdfi7j20p30fdtdr.jpg)

多master，配置文件设定，4个broker，双master、双slave。其中一个master宕机，另外一个master生产，生产能力不会停止，消费由slave负责。

**问题？**slave从master同步数据有延迟，slave从master完全同步到数据时，master宕机，master不重启，消费者在slave中消费不到

**解决：**同步复制（4个broker，两个slave都同步成功，才是写入成功）

ActiveMQ的HA通过部署BrokerA和BrokerB，以及在生产者和消费者配置Failover的URL来达成。如果BrokerA出现问题，生产和消费可以Failover到BrokerB上，但是这种模式极度依赖于BrokerA和BrokerB之间的桥接，如果桥接断开，正常的消费都无法满足。

RocketMQ的HA通过Master/Slave模式以及集群化部署达成。Slave会备份Master所有消息，如果Master出现问题，消费者可以切换到Slave上进行消费。同时生产者会将生产切换到其他可用Master进行。

### 29.消息ack和kafka比较

Kafka的Push模式消费时，后台会周期性的自动ACK消息。

如果消息到达Consumer，但是还未处理完毕，消息可能已经被ACK，如果此时Consumer重启，则消息丢失。

RocketMQ的Push模式消费时，后台同样会周期性的自动ACK消息。

但是用户可以在自定义Listener中，通过返回不同的返回值，来确认哪些消息是可以被ACK，哪些消息是消费失败需要重新Delivery的。即使Consumer重启，也不会丢失未消费的消息。

### 30.QPS

虚拟机 8核16G

```
Broker为异步复制模式时
1K的消息体，标准集群的生产消费QPS可以到50K左右（一个Master25K左右）。
10K的消息体，标准集群的生产消费QPS可以到15K左右（一个Master7K左右）。
```

物理机 48核188G

```
Broker为异步复制模式时，1K的消息体，QPS可以到17万。使用批量发送小包时，QPS可以到60万。
Broker为同步复制模式时，1K的消息体，QPS可以到1.4万。瓶颈在于等待Slave同步的网络传输时间，所以一批发多个小包可以明显提升性能，使用批量发送时，QPS可以到40万。
```



reference：https://cloud.tencent.com/developer/article/1329049

