---
layout: post
title: "老生常谈"
description: ""
category: [java,基础]
tags: [基础]
---
{% include JB/setup %}



### 浅拷贝，深拷贝

1. 对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝，此为浅拷贝。
2. **深拷贝**：对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容，此为深拷贝。

![企业微信截图_537289ee-519b-446e-a822-eeba3a156ce8.png](http://ww1.sinaimg.cn/large/87a42753ly1ggch60lbdxj20me0as7ak.jpg)

### 线程池

核心，缓冲队列，最大线程数，存活时间，拒绝策略，线程名字

缓冲队列类型

* ArrayBlockingQueue：基于数组的先进先出队列，此队列创建时必须指定大小；
* LinkedBlockingQueue：基于链表的先进先出队列，如果创建时没有指定此队列大小，则默认为Integer.MAX_VALUE
* synchronousQueue：这个队列比较特殊，它不会保存提交的任务，而是将直接新建一个线程来执行新来的任务。

四种线程池

* **newSingleThreadExecutor**

  ```
  public static ExecutorService newSingleThreadExecutor() {
      return new FinalizableDelegatedExecutorService
          (new ThreadPoolExecutor(1, 1,
                                  0L, TimeUnit.MILLISECONDS,
                                  new LinkedBlockingQueue<Runnable>()));
  }
  // corePoolSize=1,maximumPoolSize=1,keepAliveTime=0s
  ```

* **newFixedThreadPool**

  ```
  public static ExecutorService newFixedThreadPool(int nThreads) {
      return new ThreadPoolExecutor(nThreads, nThreads,
                                    0L, TimeUnit.MILLISECONDS,
                                    new LinkedBlockingQueue<Runnable>());
  }
  // corePoolSize=n,maximumPoolSize=n,keepAliveTime=0s
  ```

* **newCachedThreadPool**

  ```
  public static ExecutorService newCachedThreadPool() {
      return new ThreadPoolExecutor(0, Integer.MAX_VALUE,
                                    60L, TimeUnit.SECONDS,
                                    new SynchronousQueue<Runnable>());
  }
  // corePoolSize=0,maximumPoolSize=Integer.MAX_VALUE,keepAliveTime=60s
  ```

* **newScheduledThreadPool**

  ```
  public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) {
          return new ScheduledThreadPoolExecutor(corePoolSize);
  }
  public ScheduledThreadPoolExecutor(int corePoolSize) {
      super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS,
            new DelayedWorkQueue());
  }
  // corePoolSize=n,maximumPoolSize=Integer.MAX_VALUE,keepAliveTime=0s
  ```

线程数大小设置

最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目

### 分布式事务

* XA

  * 优缺点
    - 优点：实现简单易懂
    - 缺点：性能不理想，高并发场景下表现不佳

  * 2PC

    * 准备阶段

      事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。

    * 提交阶段

      如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)

    * 缺点（同步阻塞、单点问题、脑裂等缺陷）

      1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
      
       2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）      
      
       3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。
      
        4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

  * 3PC

    * 针对2PC的改进点

      1、引入超时机制。同时在协调者和参与者中都引入超时机制。
      2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

      也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

    * CanCommit阶段

      询问是否可以执行事务提交操作。yes or no

    * PreCommit阶段

      假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。

      假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。

    * DoCommit阶段

      在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。

* TCC

  * 优缺点

    缺点：因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常恶心。

  * 操作模式（类似2PC）

    * Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留。
    * Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。

    * Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）

* 本地消息表

  * 优缺点

    缺点：这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的，会导致如果是高并发场景咋办呢？咋扩展呢？

  * 操作模式

    * A 系统在自己本地一个事务里操作同时，插入一条数据到消息表；

    * 接着 A 系统将这个消息发送到 MQ 中去；

    * B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息；

    * B 系统执行成功之后，就会更新自己本地消息表的状态以及 A 系统消息表的状态；

    * 如果 B 系统处理失败了，那么就不会更新A的消息表状态，那么此时 A 系统会定时扫描自己的消息表，如果有未处理的消息，会再次发送到 MQ 中去，让 B 再次处理；

    * 这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。

* 可靠消息最终一致性

  * 针对本地消息表的改进点

    就是干脆不要用本地的消息表了，直接基于 MQ 来实现事务。比如阿里的 RocketMQ 就支持消息事务。

  * 优缺点

    

  * 操作模式

    * **Prepared阶段**

      该阶段主要发一个消息到rocketmq，但该消息**只储存在commitlog中**，**但consumeQueue中不可见，也就是消费端（订阅端）无法看到此消息**。

    * **确认阶段**

      该阶段主要是把prepared消息**保存到consumeQueue中**，即**让消费端可以看到此消息**，也就是**可以消费此消息**。

    * 问题场景和解决方案

      * **异常1：**如果发送预备消息失败，下面的流程不会走下去；
      * **异常2：**如果发送预备消息成功，但执行本地事务失败；这个也没有问题，**因为此预备消息不会被消费端订阅到，消费端不会执行业务。**
      * **异常3：**如果发送预备消息成功，执行本地事务成功，但发送确认消息失败；这个就有问题了，因为用户A扣款成功了，但加钱业务没有订阅到确认消息，无法加钱。**这里出现了数据不一致。**

      RocketMq如何解决上面的问题，**核心思路就是【状态回查】**，也就是RocketMq会定时遍历commitlog中的预备消息。（解决了异常2和3，异常2是发rollback删除预备消息，异常3是补发确认消息）

      回查业务是否成功：**设计一张Transaction表**，将**业务表和Transaction绑定在同一个本地事务中**，如果扣款本地事务成功时，Transaction中应当已经记录该TransactionId的状态为「已完成」。当RocketMq回查时，只需要检查对应的**TransactionId的状态是否是「已完成」就好**，而不用关心具体的业务数据。

* 最大努力通知

  * 操作模式
    * 系统 A 本地事务执行完之后，发送个消息到 MQ；

    * 这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口；

    * 要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。